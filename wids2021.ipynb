{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import StratifiedKFold\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n    Helper Function's\n'''\ndef get_roc_auc_score(Model, X, y):\n    predict = Model.predict(X)\n    predict = list(predict.argmax(1))\n    return roc_auc_score(y, predict)\n\ndef get_missing_vitals_df(df):\n    vital = [v.split('d1_')[1].split('_max')[0] for v in df[(df.str.startswith('d1')&df.columns.str.endswith('max'))]]\n    vitals = []\n    vitals_h1_max = []\n    vitals_h1_min = []\n    vitals_d1_max = []\n    vitals_d1_min = []\n    for v in vital:\n        vitals.append(v)\n        vitals_h1_max.append(f'h1_{v}_max')\n        vitals_h1_min.append(f'h1_{v}_min')\n        vitals_d1_max.append(f'd1_{v}_max')\n        vitals_d1_min.append(f'd1_{v}_min')\n\n    temp = pd.DataFrame({\n        'vitals' : vitals,\n        'vitals_d1_max' : df[vitals_d1_max].isna().sum().values,\n        'vitals_d1_min' : df[vitals_d1_min].isna().sum().values,\n        'vitals_h1_max' : df[vitals_h1_max].isna().sum().values,\n        'vitals_h1_min' : df[vitals_h1_min].isna().sum().values}\n    ).set_index('vitals')\n    return temp\n\ndef gelu(x):\n    return 0.5*x*(1 + tf.tanh(tf.sqrt(2 / np.pi)*(x + 0.044715 * tf.pow(x,3))))\n\ndef clean_data(df):\n    df = df.drop(columns = ['Unnamed: 0','encounter_id','hospital_id','icu_id'])\n    col_cat = []\n    for col in df.columns:\n        if(df.dtypes[col]=='object'):\n            df[col] = df[col].fillna('Na')\n            df[col] = df[col].astype('str')\n            col_cat.append(col)\n        elif(Column_Dt_Mapping[col]=='binary'):\n            df[col] = df[col].fillna('2')\n            df[col] = df[col].astype('str')\n            col_cat.append(col)\n    for col in ['apache_3j_diagnosis','apache_2_diagnosis']:\n        col_cat.append(col)\n        df[col] = df[col].fillna('Na')\n        df[col] = df[col].astype('str')\n    \n    for col in ['height', 'weight', 'bmi']:\n        df[col] = np.where((df[col].isna() & (df.gender == 'M')),df[df['gender']=='M'][col].mean(),df[col])\n        df[col] = np.where((df[col].isna() & (df.gender == 'F')),df[df['gender']=='F'][col].mean(),df[col])\n        df[col] = np.where((df[col].isna() & (df.gender == 'Na')),df[df['gender']=='Na'][col].mean(),df[col])\n\n    invasive_min_max = [v.split('h1_')[1].split('_invasive_min')[0] for v in df.columns[(df.columns.str.startswith('h1_') & df.columns.str.endswith('_invasive_min'))]]\n\n    for col in invasive_min_max:\n        df[f'h1_{col}_max'].fillna(df[f'h1_{col}_max'].median(),inplace=True)\n        df[f'h1_{col}_min'].fillna(df[f'h1_{col}_min'].median(),inplace=True)\n        df[f'd1_{col}_max'].fillna(df[f'd1_{col}_max'].median(),inplace=True)\n        df[f'd1_{col}_min'].fillna(df[f'd1_{col}_min'].median(),inplace=True)\n        df[f'h1_{col}_invasive_max'] = np.where(df[f'h1_{col}_invasive_max'].isna(),df[f'h1_{col}_max'],df[f'h1_{col}_max'])\n        df[f'h1_{col}_invasive_min'] = np.where(df[f'h1_{col}_invasive_min'].isna(),df[f'h1_{col}_min'],df[f'h1_{col}_min'])\n        df[f'h1_{col}_noninvasive_max'] = np.where(df[f'h1_{col}_noninvasive_max'].isna(),df[f'h1_{col}_max'],df[f'h1_{col}_max'])\n        df[f'h1_{col}_noninvasive_min'] = np.where(df[f'h1_{col}_noninvasive_min'].isna(),df[f'h1_{col}_min'],df[f'h1_{col}_min'])\n        df[f'd1_{col}_invasive_max'] = np.where(df[f'd1_{col}_invasive_max'].isna(),df[f'd1_{col}_max'],df[f'd1_{col}_max'])\n        df[f'd1_{col}_invasive_min'] = np.where(df[f'd1_{col}_invasive_min'].isna(),df[f'd1_{col}_min'],df[f'd1_{col}_min'])\n        df[f'd1_{col}_noninvasive_max'] = np.where(df[f'd1_{col}_noninvasive_max'].isna(),df[f'd1_{col}_max'],df[f'd1_{col}_max'])\n        df[f'd1_{col}_noninvasive_min'] = np.where(df[f'd1_{col}_noninvasive_min'].isna(),df[f'd1_{col}_min'],df[f'd1_{col}_min'])\n\n    for col in ['albumin','bilirubin','creatinine','glucose','hematocrit','resprate','sodium','temp','wbc','bun']:\n        df[f'd1_{col}_max'].fillna(df[f'd1_{col}_max'].median(),inplace=True)\n        df[f'd1_{col}_min'].fillna(df[f'd1_{col}_min'].median(),inplace=True)\n        df[f'h1_{col}_max'] = np.where(df[f'h1_{col}_max'].isna(),df[f'd1_{col}_max'],df[f'h1_{col}_max'])\n        df[f'h1_{col}_min'] = np.where(df[f'h1_{col}_min'].isna(),df[f'd1_{col}_min'],df[f'h1_{col}_min'])\n        df[f'{col}_apache'] = np.where(df[f'{col}_apache'].isna(),df[f'd1_{col}_max'],df[f'{col}_apache'])\n\n    for col in ['spo2','calcium','hco3','hemaglobin','inr','lactate','platelets','potassium','arterial_pco2','arterial_ph','arterial_po2','heartrate','pao2fio2ratio']:\n        df[f'd1_{col}_max'].fillna(df[f'd1_{col}_max'].median(),inplace=True)\n        df[f'd1_{col}_min'].fillna(df[f'd1_{col}_min'].median(),inplace=True)\n        df[f'h1_{col}_max'] = np.where(df[f'h1_{col}_max'].isna(),df[f'd1_{col}_max'],df[f'h1_{col}_max'])\n        df[f'h1_{col}_min'] = np.where(df[f'h1_{col}_min'].isna(),df[f'd1_{col}_min'],df[f'h1_{col}_min'])    \n\n    for col in ['fio2','gcs_eyes','gcs_motor','gcs_verbal','heart_rate','map','urineoutput','heart_rate','paco2','paco2_for_ph','pao2','ph']:\n        df[f'{col}_apache'].fillna(df[f'{col}_apache'].median(),inplace=True)\n\n    return col_cat, df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Base_dir = '../input/widsdatathon2021/'\ntrain = pd.read_csv(os.path.join(Base_dir+'TrainingWiDS2021.csv'))\ntest = pd.read_csv(os.path.join(Base_dir+'UnlabeledWiDS2021.csv'))\nDataDictionary = pd.read_csv(os.path.join(Base_dir,'DataDictionaryWiDS2021.csv'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Imputing Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_clean = test.copy()\ntrain_clean = train.copy()\n\nColumn_Dt_Mapping = dict(zip(DataDictionary['Variable Name'],DataDictionary['Data Type']))\n\ntest_clean.at[8360,'apache_3j_diagnosis'] = np.nan   \ntest_clean.at[8494,'apache_3j_diagnosis'] = np.nan   \n\ntrain_clean['age'] = np.where(train_clean['age'].isna(),train_clean['age'].median(),train_clean['age'])\ntest_clean['age'] = test_clean['age'].astype('float64')\n\n_, train_clean = clean_data(train_clean)\ncol_cat, test_clean = clean_data(test_clean)\n\nfor col in col_cat:\n    le = LabelEncoder().fit(train_clean[col])\n    train_clean[col] = le.transform(train_clean[col])\n    test_clean[col] = le.transform(test_clean[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_columns_tr = {k: v for k, v in train_clean.isna().sum().items() if v}\nmissing_columns_te = {k: v for k, v in test_clean.isna().sum().items() if v}\nprint(f'No. of columns having missing items in training columns: {len(missing_columns_tr)}, in test columns: {len(missing_columns_te)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vt = VarianceThreshold(threshold=0.001)\n#vt.fit_transform(train_clean).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train_clean.columns:\n    if((train_clean[col].nunique() == 1)and(train_clean[col].dtype!='float64')):\n        print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_clean = train_clean.drop('readmission_status',1)\ntest_clean = test_clean.drop('readmission_status',1)\ncol_cat.remove('readmission_status')\ncol_cont = [col for col in train_clean.columns if col not in col_cat]\ncol_cont.remove('diabetes_mellitus')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaler = StandardScaler()\n#X[col_cont] = scaler.fit_transform(X[col_cont])\n#X_valid[col_cont] = scaler.fit_transform(X_valid[col_cont])\n#test_data = test_clean.copy()\n#test_data[col_cont] = scaler.transform(test_data[col_cont])\n\n\n#X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42, stratify = y)\n\n#scaler = StandardScaler()\n#X_train[col_cont] = scaler.fit_transform(X_train[col_cont])\n#X_valid[col_cont] = scaler.fit_transform(X_valid[col_cont])\n#test_data = test_clean.copy()\n#test_data[col_cont] = scaler.transform(test_data[col_cont])\n\n#X_train = [np.absolute(X_train.loc[:,f]) for f in col_cat]+[X_train[col_cont]]\n#X_valid = [np.absolute(X_valid.loc[:,f]) for f in col_cat]+[X_valid[col_cont]]\n#test_data = [np.absolute(test_data.loc[:,f]) for f in col_cat]+[test_data[col_cont]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Model, load_model\nfrom keras.layers import Dropout, Dense, BatchNormalization, Embedding, Input, Concatenate, SpatialDropout1D, Reshape, Flatten, concatenate, Activation, LeakyReLU\nfrom keras.metrics import AUC\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.utils import get_custom_objects\nfrom keras.optimizers import Adam, SGD\nfrom keras.losses import binary_crossentropy\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nget_custom_objects().update({'gelu': Activation(gelu)})\nget_custom_objects().update({'leaky-relu': Activation(LeakyReLU(alpha=0.2))})\n\n\n\nCFG = {\n    'feature_selection_dropout' : 0.2,\n    'categorical_dropout' : 0.1,\n    'first_dense' : 256,\n    'second_dense' : 256,\n    'dense_dropout' : 0.2,\n    'activation_type' : 'leaky-relu',\n    'activation' : 'sigmoid',\n    'epochs' : 200,\n    'loss' : 'binary_crossentropy',\n    'optimizer' : Adam(learning_rate=0.001),\n    'mon_metrics' : 'val_auc',\n    'num_folds' : 5\n      }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dnn_embedding(feature_selection_dropout = CFG['feature_selection_dropout'], categorical_dropout = CFG['categorical_dropout'],\n                first_dense = CFG['first_dense'], second_dense = CFG['second_dense'], dense_dropout = CFG['dense_dropout'], \n                activation_type = CFG['activation_type'], activation = CFG['activation']):\n\n    inputs = []\n    embeddings = []\n\n    for category in  col_cat:\n        categorical_inputs = Input(shape=[1], name=category)\n        num_unique_vals = int(train_clean[category].nunique())\n        embed_dim = int(min(np.ceil(num_unique_vals / 2), 100)) \n        categorical_outputs = Embedding(num_unique_vals+1, \n                      embed_dim, \n                      name = category + \"_embed\")(categorical_inputs)\n        categorical_outputs = SpatialDropout1D(categorical_dropout)(categorical_outputs)\n        categorical_outputs = Reshape(target_shape=(embed_dim,))(categorical_outputs)\n        inputs.append(categorical_inputs)\n        embeddings.append(categorical_outputs)\n        \n        \n    numerical_inputs = Input(shape=(len(col_cont),))\n    numerical_normalization = BatchNormalization()(numerical_inputs)\n    #numerical_feature_selection = Dropout(feature_selection_dropout)(numerical_normalization)\n    inputs.append(numerical_inputs)\n    embeddings.append(numerical_normalization)    \n    \n    x = concatenate(embeddings)\n    x = Dense(first_dense, activation=activation_type)(x)\n    #x = BatchNormalization()(x)\n    x = Dropout(dense_dropout)(x)  \n    x = Dense(second_dense, activation=activation_type)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(dense_dropout)(x)\n    x = Dense(second_dense, activation=activation_type)(x)\n    #x = BatchNormalization()(x)\n    x = Dropout(dense_dropout)(x)\n    x = Dense(second_dense, activation=activation_type)(x)\n    #x = BatchNormalization()(x)\n    x = Dropout(dense_dropout)(x)\n    y = Dense(1, activation=activation)(x)\n    model = Model(inputs = inputs , outputs = y)\n    \n    return model ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dnn_embedding().summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_preds = np.zeros((len(train)))\ntest_preds = np.zeros((len(test)))\n\ny = train_clean['diabetes_mellitus']\ny = y.astype(int)\n#train_clean = train_clean.drop('diabetes_mellitus',1)\nX = train_clean.drop('diabetes_mellitus',1)\n\nskf = StratifiedKFold(n_splits=CFG['num_folds'], shuffle = True, random_state=42)\n\nfor folds, (tdx, vdx) in enumerate(skf.split(X, y.values)):\n    X_train, X_valid, y_train, y_valid = X.iloc[tdx], X.iloc[vdx], y[tdx], y[vdx]\n\n    scaler = StandardScaler()\n    X_train[col_cont] = scaler.fit_transform(X_train[col_cont])\n    X_valid[col_cont] = scaler.transform(X_valid[col_cont])\n    test_data = test_clean.copy()\n    test_data[col_cont] = scaler.transform(test_data[col_cont])\n    \n    test_data = [np.absolute(test_data[col]) for col in col_cat] + [test_data[col_cont]]\n    X_train = [np.absolute(X_train[col]) for col in col_cat] + [X_train[col_cont]]\n    X_valid = [np.absolute(X_valid[col]) for col in col_cat] + [X_valid[col_cont]]\n    \n    model = dnn_embedding()\n\n\n    model.compile(loss = CFG['loss'], optimizer = CFG['optimizer'], metrics = ['accuracy','AUC'])\n\n    es = EarlyStopping(monitor = CFG['mon_metrics'], min_delta = 0.001, patience = 20,\n                    verbose = 1, mode = 'max', baseline = None, restore_best_weights = True)\n\n    rlr = ReduceLROnPlateau(monitor=CFG['mon_metrics'], factor = 0.5, patience=5, mode='max', verbose=1, min_lr = 1e-6)\n    \n                                        \n    model.fit(X_train,\n              y_train,\n              validation_data = (X_valid, y_valid),\n              verbose = 1,\n              batch_size = 5024,\n              callbacks=[es],\n              epochs=CFG['epochs']\n             )\n\n    valid_fold_preds = model.predict(X_valid)\n    test_fold_preds = model.predict(test_data)\n    oof_preds[vdx] = valid_fold_preds.ravel()\n    test_preds += test_fold_preds.ravel()\n    print(f'Fold: {str(folds)}, AUC: {roc_auc_score(y_valid, valid_fold_preds)}')\n    #K.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUC_FINAL = roc_auc_score(y.values, oof_preds)\ntest_predictions = test_preds/(folds+1)\nprint(f'Overall AUC ROC: {AUC_FINAL}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Unlabeled1 = pd.read_csv(os.path.join(Base_dir,'UnlabeledWiDS2021.csv'))\nsubmit = Unlabeled1[['encounter_id']]\nsubmit['diabetes_mellitus'] = test_predictions\nsubmit.to_csv('ann_embeded_complete_epoch_02_00.csv',index=False)\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}